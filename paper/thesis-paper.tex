\documentclass[ms,electronic,twosidetoc,letterpaper,chaptercenter,parttop,lol,lof,lot]{byumsphd}
\usepackage{slatex}
% Author: Chris Monson
%
% This document is in the public domain
%
% Options for this class include the following (* indicates default):
%
%   phd (*) -- produce a dissertation
%   ms -- produce a thesis
%
%   electronic -- default official university option, overrides the following:
%                 - equalmargins
%
%   hardcopy -- overrides the following:
%                 - no equalmargins
%                 - twoside
%
%   letterpaper -- ignored, but helpful for the Makefile that I use
%
%   10pt -- 10 point font size
%   11pt -- 11 point font size
%   12pt (*) -- 12 point font size
%
%   lof -- produce a list of figures in the preamble (off)
%   lot -- produce a list of tables in the preamble (off)
%   lol -- produce a list of listings in the preamble (off)
%
%   layout -- show layout lines on the pages, helps with overfull boxes (off)
%   grid -- show a half-inch grid on every page, helps with printing (off)
%   separator -- print an extra instruction page between preamble and body (off)
%
%   twoside (*) -- two-sided output (margins alternate for odd and even pages,
%     blank pages inserted to ensure that chapters begin on the right side of a
%     bound copy, etc.)
%   oneside -- one-sided output (margins are the same on all pages)
%   equalmargins -- make all margins equal - ugly for binding, but compliant
%
%   twosidetoc - start two-sided margins at the TOC instead of the body.  This
%     is sometimes (oddly) required, but be aware that it will make the page
%     numbering seem screwy, e.g., the first four full sheets of paper will
%     have number i-iv (not shown, though), and the next sheets will each have
%     two numbers, one for each side.  I suspect that most people don't look at
%     the roman numerals anyway, but it is a weird requirement.
%
%   openright (*) -- force new chapters to start on an odd page
%   openany -- don't use this, it's ugly
%
%   prettyheadings -- make the section/chapter headings look nice
%   compliantheadings (*) -- make them look ugly, but compliant with standards
%
%   chaptercenter -- center the chapter headings horizontally
%   chapterleft (*) -- place chapter headings on the left
%
%   partmiddle -- Part headers are centered vertically, no other text on page
%   parttop (*) -- Part headers at top of page, other text expected
%
%   duplexprinter -- Ensures that the two-sided portion starts on the right
%     side when printing.  This is not for use in submission, since the best
%     thing to do there is to print everything out one-sided, then take it down
%     to the copy store to have them do the rest.  It does help to save trees
%     when you are printing out copies just to look at them and fiddle with
%     things.
%
%
% EXAMPLES:
%
% The rest is up to you.  To fiddle with margins, use the \settextwidth and
% \setbindingoffset macros, described below.  I suggest that you
% \settextwidth{6.0in} for better-looking output (otherwise you'll get 3/4-inch
% margins after binding, which is sort of weird).  This will depend on the
% opinions of the various dean/coordinator folks, though, so be sure to ask
% them before embarking on a major formatting task.

% The following command fixes my particular printer, which starts 0.03 inches
% too low, shifting the whole page down by that amount.  This shifts the
% document content up so that it comes out right when printed.
%
% Discovering this sort of behavior is best done by specifying the ``grid''
% option in the class parameters above.  It prints a 1/2 inch grid on every
% page.  You can then use a ruler to determine exactly what the printer is
% doing.
%
% Uncomment to shift content up (accounting for printer problems)
%\setlength{\voffset}{-.03in}

% Here we set things up for invisible hyperlinks in the document.  This makes
% the electronic version clickable without changing the way that the document
% prints.  It's useful, but optional.
%
% NOTE: "driverfallback=ps2pdf" chooses ps2pdf in the case of LaTeX and pdftex
% in the case of pdflatex. If you use my LaTeX makefile (at
% http://latex-makefile.googlecode.com/) then pdftex is the default There are
% many other benefits to using the makefile, too.  This option is not always
% available, so use with care.
\usepackage[
    bookmarks=true,
    bookmarksnumbered=true,
    breaklinks=false,
    raiselinks=true,
    pdfborder={0 0 0},
    colorlinks=false,
    plainpages=false,
    ]{hyperref}

% To fiddle with the margin settings use the below.  DO NOT change stuff
% directly (like setting \textwidth) - it will break subtle things and you'll
% be tearing your hair out.
%
% For example, if you want 1.5in equal margins, or 2in and 1in margins when
% printing, add the following below:
%
%\setbindingoffset{1.0in}
%\settextwidth{5.5in}
%
% When equalmargins is specified in the class options, the margins will be
% equal at 1.5in each: (8.5 - 5.5) / 2.  When equalmargins is not specified,
% the inner margin will be 2.0 and the outer margin will be 1.0: inner = (8.5 -
% 5.5 - 1.0) / 2 + 1.0 (the 1.0 is the binding offset).
%
% The idea is this: you determine how much space the text is going to take up,
% whether for an electronic document (equalmargins) or not.  You don't want the
% layout shifting around between printed and electronic documents.
%
% So, you specify the text width.  Then, if there is a binding offset (when
% binding your thesis, the binding takes up space - usually 0.5 inches), that
% reduces the visual space on the final printed copy.  So, the *effective*
% margins are calculated by reducing the page size by the binding offset, then
% computing the remaining space and dividing by two.  Adding back in the
% binding offset gives the inner margin.  The outer margin is just what's left.
%
% All of this is done using the geometry package, which should be manipulated
% directly at your peril.  It's best just to use the above macros to manipulate
% your margins.
%
% That said, using the geometry macro to set top and bottom margins, or
% anything else vertical, is perfectly safe and encouraged, e.g.,
%
%\geometry{top=2.0in,bottom=2.0in}
%
% Just don't fiddle with horizontal margins this way.  You have been warned.

% This makes hyperlinks point to the tops of figures, not their captions
\usepackage[all]{hypcap}

% These packages allow the bibliography to be sorted alphabetically and allow references to more than one paper to be sorted and compressed (i.e. instead of [5,2,4,6] you get [2,4-6])
\usepackage[numbers,sort&compress]{natbib}
\usepackage{hypernat}

% Because I use these things in more than one place, I created new commands for
% them.  I did not use \providecommand because I absolutely want LaTeX to error
% out if these already exist.
\newcommand{\Title}{Autocompletion for the Rest of Us}
\newcommand{\Author}{Nick Shelley}
\newcommand{\GraduationMonth}{August}
\newcommand{\GraduationYear}{2014}

% Set up the internal PDF information so that it becomes part of the document
% metadata.  The pdfinfo command will display this.
\hypersetup{%
    pdftitle=\Title,%
    pdfauthor=\Author,%
    pdfsubject={Masters Thesis, BYU CS Department: %
                Degree Granted \GraduationMonth~\GraduationYear, Document Created \today},%
    pdfkeywords={BYU, thesis, dissertation, LaTeX},%
}

% Rewrite the itemize, description, and enumerate environments to have more
% reasonable spacing:
\newcommand{\ItemSep}{\itemsep 0pt}
\let\oldenum=\enumerate
\renewcommand{\enumerate}{\oldenum \ItemSep}
\let\olditem=\itemize
\renewcommand{\itemize}{\olditem \ItemSep}
\let\olddesc=\description
\renewcommand{\description}{\olddesc \ItemSep}

% Important settings for the byumsphd class.
\title{\Title}
\author{\Author}

\committeechair{Jay~McCarthy}
\committeemembera{Bryan~Morse}
\committeememberb{Dennis~Ng}

\monthgraduated{\GraduationMonth}
\yeargraduated{\GraduationYear}
\yearcopyrighted{\GraduationYear}

\documentabstract{%
Code completion systems act both as a way to decrease typing and as a way to increase awareness. The former is typically done by completing known variable or function names, while the latter is done by providing a list of possible completions or by providing documentation. Static type information makes these goals possible and feasible for qualifying languages, so most work in this area is focused on improving the order of results or trimming less-valuable results. It follows that almost all validation techniques for this work have focused on proving how well a completion system can put a desired result at the top of the list. However, because of the lack of static type information in dynamically-typed languages, achieving the aforementioned goals is much harder, and many of the completion suggestions may even result in compile-time or runtime crashes. Unfortunately, of the work done on creating completers for these languages, little validation work has been done, making it hard to determine what improvements can be made. This thesis will provide two validation techniques that will provide information both on how well completion suggestions are ordered and also which completion suggestions result in errors. This information will be used to guide the development and evolution of a completion system for the Racket programming language.
}

\documentkeywords{%
    code completion, auto-complete, autocomplete
}

\acknowledgments{%
    Thanks to Dr. McCarthy for his support and patience.
}

\department{Computer~Science}
\graduatecoordinator{Quinn~Snell}
\collegedean{Thomas~W.~Sederberg}
\collegedeantitle{Associate~Dean}

% Customize the name of the Table of Contents section.
\renewcommand\contentsname{Table of Contents}

% Remove all widows an orphans.  This is not normally recommended, but in a
% paper dissertation there is no reasonable way around it; you can't exactly
% rewrite already-published content to fix the problem.
\clubpenalty 10000
\widowpenalty 10000

% Allow pages to have extra blank space at the bottom in order to accommodate
% removal of widows and orphans.
\raggedbottom

% Produce nicely formatted paragraphs. There is nothing additional to do.  In
% case you get some problems, surround your text with
% \begin{sloppy} ... \end{sloppy}. If that does not work, try
% \microtypesetup{protrusion=false} ... \microtypesetup{protrusion=true}
\usepackage{microtype}

\begin{document}

% Produce the preamble
\microtypesetup{protrusion=false}
\maketitle
\microtypesetup{protrusion=true}

\chapter{Introduction}

The main purpose of autocompletion is to improve programmer productivity. This has mainly been accomplished in two ways. The first way is to reduce typing by predicting which token(s) should be inserted at a given position, sometimes replacing the partially-typed token. Incidentally, this also encourages more descriptive variable names since longer names can be completed automatically. The second way is to provide convenient documentation to the programmer. For example, a list of currently valid (i.e. in scope, correctly typed, etc.) substitutions may be provided given the current cursor position. Also, a function's signature and purpose can be displayed next to the typed function name. This project will focus on the first way of completing textual tokens rather than providing convenient documentation.

Autocompletion for statically-typed languages is relatively simple to do naively. Since type information is available statically, it is easy to avoid suggesting tokens that will result in compile or runtime errors. Because static type information makes it possible to suggest completions with high assurance of being valid, research and work has focused mainly on presenting these valid results in a way that is more useful to the programmer. For instance, instead of giving all valid results in alphabetical order, results can be omitted or promoted closer to the top of the list based on context, type, or analysis of previous code. Therefore, validation systems have mainly focused on how well a desired token makes it in the top N suggestions.

However, because type information is not known statically in other languages, it is possible and easy to suggest tokens that may result in runtime errors and others that may not even compile. Other features that make autocompletion hard for certain languages include first-class functions and powerful macro systems. I will now discuss the challenges these features pose in detail.

% set up slatex stuff
\setkeyword{racket require for-syntax provide}

\section{Dynamic Typing}

Dynamically typed languages do the majority of their type checking at run-time as opposed to compile-time. In dynamic typing, values have types but variables do not. A variable can thus refer to any value of any type at any time.

Because a variable's type can change at any time in the program, the type information can't be used to filter which variables can appear in which contexts. For example, consider the incomplete programs in Figure~\ref{fig:dynamic-type}. Should an auto-completer give the parameter \scheme{a} as an option to fill the hole in \texttt{dynamic-parameter.rkt}?  With the call \scheme{(add2 5)}, the program would have a meaningful result. However, with the call \scheme{(add2 #t)}, there would be a run-time error. Since the auto-completer can't know what types of values the parameter \scheme{a} will be at run-time, it cannot know whether it is correct to include it in the auto-complete list.

As another example, consider the two holes in \texttt{dynamic-return-value.rkt}. The auto-completer needs to make a decision about whether or not to include \scheme{num} in the list of suggestions. If \scheme{num} filled the first hole, the number \scheme{10} would be inserted and the result of the program would be \scheme{15}. However, if \scheme{num} filled the second hole, the string \scheme{"hi"} would be inserted and a run-time error would occur. Although it is easy for us as humans to look at these two function calls and know what they will return, it is hard for a computer to do so without running the program first. We can also imagine more complicated functions which would be hard for both humans and computers to analyze. Since the auto-completer can't even know what type a function will return, it cannot know whether it is correct to include the result of a function in the auto-complete list.

\begin{figure*}[t]
\hrule
\centering
\renewcommand{\arraystretch}{2}
\begin{tabular}{c@{\hspace{0.2\linewidth}}c}
\texttt{dynamic-parameter.rkt}
&
\texttt{dynamic-return-value.rkt}
\\
\begin{minipage}[t]{\linewidth}
\begin{schemedisplay}
#lang racket
(define (add2 a)
  (+ [ ] 2))
\end{schemedisplay}
\end{minipage}
&
\begin{minipage}[t]{\linewidth}
\begin{schemedisplay}
#lang racket
(define (return-something a)
  (if (< a 5) 10 "hi"))

(define num (return-something 2))
(+ [ ] 5)
(set! num (return-something 8))
(+ [ ] 5)
\end{schemedisplay}
\end{minipage}
\\
\end{tabular}
\vspace{0.5cm}
\hrule
\caption{Problems with Dynamic Typing} \label{fig:dynamic-type}
\end{figure*}

\section{First-class Functions}

First-class functions allow functions to be passed around as values. Coupled with the dynamic typing problem, first-class functions make it impossible to know which bindings are functions and which aren't. Ideally, if an auto-completer is invoked where it makes sense to call a function (such as after an opening parenthesis), only functions should appear in the list of suggestions. However, because we can't know which bindings are functions, we must consider all bindings. For example, the program \texttt{function-parameter.rkt} in Figure~\ref{fig:first-class-functions} contains two holes, one where a boolean should go, and another where a function should go. Because passing \scheme{+} to \scheme{fun} is just as valid as passing \scheme{#t} or \scheme{3}, we can't know if it is correct to include the parameter \scheme{f} in either of those holes.

Even if we could know or assume that a function is being called, we cannot know which function a variable is bound to. This makes bringing up information about the function, such as a parameter list, impossible. For example, the program \texttt{function-return.rkt} in the same figure contains two holes. In the first instance we would want the auto-completer to bring up the possible parameters to the \scheme{+} function that will be returned, which are \scheme{z ...}. In the second instance we would want the auto-completer to bring up \scheme{make-vector}'s possible parameters, which are \scheme{size [v]}. However, because we don't know what function will be returned, we cannot bring up the correct parameters. 

\begin{figure*}[t]
\hrule
\centering
\renewcommand{\arraystretch}{2}
\begin{tabular}{c@{\hspace{0.2\linewidth}}c}
\texttt{function-parameter.rkt}
&
\texttt{function-return.rkt}
\\
\begin{minipage}[t]{\linewidth}
\begin{schemedisplay}
#lang racket
(define (fun f)
  (when [ ] ([ ] 3 2))
\end{schemedisplay}
\end{minipage}
&
\begin{minipage}[t]{\linewidth}
\begin{schemedisplay}
#lang racket
(define (return-fun a)
  (if (< a 5) + make-vector))

((return-fun 2) [ ])
((return-fun 7) [ ])
\end{schemedisplay}
\end{minipage}
\\
\end{tabular}
\vspace{0.5cm}
\hrule
\caption{Problems with First-class Functions} \label{fig:first-class-functions}
\end{figure*}

\section{Powerful Macro System}

Racket macros are essentially an API to add compiler extensions. All macros compile down to a small set of core language features, potentially going through intermediate transformations when other macros are used in macro definitions. Most of the features available in \scheme{#lang racket} are macro definitions. For example, the \scheme{and} and \scheme{or} constructs are actually macros that compile to the core \scheme{let} and \scheme{if} constructs. Macros such as \scheme{or} can be used to define other macros, which can be used to define yet more macros. A program is said to be fully expanded when all macros have been recursively transformed until only core language constructs remain.

Syntax objects are an important part of the Racket macro system. Syntax objects basically wrap source code with lexical and location information. For example, a syntax object for the symbol \scheme{x} could have lexical information describing which bindings are visible to \scheme{x}.

Racket macros are transformers that take a syntax object and return a syntax object, or in other words, they take source code and return source code. The syntax object returned can do anything it wants with the syntax object passed in, or it could ignore its input and return an arbitrary syntax object. In other words, Racket macros can do arbitrary things to the program.

Figure~\ref{fig:macros} shows macros that can cause problems for an auto-completer. In \texttt{binding.rkt}, the \scheme{not-define} macro introduces a binding for the variable \scheme{x}, breaking its promise not to define anything. It does this by extracting the value from the syntax object passed in and creating a new syntax object that defines \scheme{x} as this value. However, by examining the program, an auto-completer has no indication that \scheme{x} has been defined anywhere. In particular, at the hole in the \scheme{+} application in \texttt{binding.rkt}, the auto-completer is not aware that \scheme{x} is an available binding.

In \texttt{context.rkt}, \scheme{x} is bound to \scheme{5} within the \scheme{let}. By examining the source code, an auto-completer could give \scheme{x} as an option at the hole within the \scheme{let}. However, in this case the addition expression is passed to the \scheme{f} macro. This macro essentially replaces the context of the expression passed in with the context of \scheme{anchor}. Since \scheme{anchor} was defined at the top-level, the expression returned from \scheme{f} will only have access to top-level bindings. This can cause two problems. As we see in \texttt{context.rkt}, \scheme{x} is bound to \scheme{#f} at the top level, so filling the hole with \scheme{x} would cause run-time error to occur. Even an auto-completer that was smart enough to get around the dynamic typing problem by only including bindings of the correct type would be fooled by this scenario. Thus the first problem is essentially a complication of the dynamic typing problem. However, if we remove the definition of \scheme{x} at the top-level, then there would be no binding for \scheme{x} available to the expression, which would cause a compile-time error.

A possible work-around for macro-related problems could be to fully expand the program before invoking the auto-completer. However, it is currently impossible to expand incomplete programs. Some possible ways around this problem could be to judiciously choose points in the program at which to attempt expansion or to reduce the current program to a complete expandable program. My project will help to show how useful these approximations will be.

\begin{figure*}[t]
\hrule
\centering
\renewcommand{\arraystretch}{2}
\begin{tabular}{c@{\hspace{0.2\linewidth}}c}
\texttt{binding.rkt}
&
\texttt{context.rkt}
\\
\begin{minipage}[t]{\linewidth}
\begin{schemedisplay}
#lang racket
(define-syntax (not-define stx)
  (syntax-case stx ()
    [(_ v)
     (with-syntax ([id (datum->syntax stx 'x)])
       (syntax
        (define id v)))]))

(not-define 6)
(+ [ ] 2)
\end{schemedisplay}
\end{minipage}
&
\begin{minipage}[t]{\linewidth}
\begin{schemedisplay}
#lang racket
(require (for-syntax syntax/strip-context))
(define-for-syntax anchor #'here)
(define-syntax (f stx)
  (syntax-case stx ()
    [(_ e)
     (replace-context anchor #'e)]))

(define x #f)
(let ([x 5])
  (f (+ [ ] 2)))
\end{schemedisplay}
\end{minipage}
\\
\end{tabular}
\vspace{0.5cm}
\hrule
\caption{Problems with Macros} \label{fig:macros}
\end{figure*}

\paragraph{} As shown, these language features make it especially hard to implement completion systems for such languages. Because of this, work in this area has focused on providing suggestions in a similar way to static completers. Because validation efforts for such completers are minimum, improvements to such systems relies on user complaints, and implementing new systems correctly is hard at best.

\chapter{Related Work}

Some commercial and research completion systems have been implemented for dynamically-typed languages such as PHP and JavaScript \cite{Mapping, Semantic, Vs.php, JScript}. These systems use knowledge about the structure of the language to complete common constructs such as functions, objects, and variables. Dynamic information is either ignored or else semantic analysis and type inference is used to statically determine as much information as possible. None of these systems provide or document any sort of testing or validation framework.

For traditional completion systems, many possible improvements have been explored over the traditional alphabetical sorting of all possible suggestions. \cite{Jungloid} uses intimate knowledge about the Java type system, library and API functions, example code, and the current context of the program to suggest ways to get from one type to another type through intermediate steps. \cite{BCC} sorts by Java type starting with methods provided from the declared type and then up the type hierarchy until the base Object class. It also allows for custom filters to be specified by the programmer. \cite{Abbreviated} takes abbreviated input instead of prefixes and uses a hidden markov model to determine what was meant. For example, the expression "ch.opn(n)" would turn into "chooser.showOpenDialog(null)." \cite{History} tracks all changes to the codebase and prioritizes suggestions based on how recently methods and objects have been changed. \cite{Examples} uses existing code (such as the Eclipse code base) as training data to implement three different completion systems. The first orders suggestions based on how frequently they are used. The second uses limited context information to create association rules that govern suggestion order. For instance, one rule might be that after objects are created, suggest setters on those objects. The third gathers context and uses a modification of the k-nearest-neighbor algorithm to order suggestions based on the nearest snippets found in the training data.

Most validation techniques ran their systems on modified existing code and ranked the results in some way. \cite{Abbreviated} ran their system on 3000 lines of code from 6 open source projects that were turned into acronym-like abbreviations. They measured how many lines could be completed and how many of those were in the top-N (i.e. top-1 or top-5) suggested completions. \cite{Towards} ran their completion system at every Swing method in several open source projects. They ranked how high in the suggestions list the actual method appeared and added all rankings together to get an overall ranking for the system. \cite{History} ran their completion system between every recorded change operation (defined as a change in the AST) on a project of theirs called SpyWare. They then recorded what percentage of actual methods were in the top-N suggestions. Lastly, \cite{Examples} used the Eclipse code base and randomly split the code into 90\% training data and 10\% test data. They then took out half of the Swing method calls in the test data and ran their completion system in each hole. They measured recall, which is how often a used method was in the suggestion list, and precision, which is what fraction of suggested methods were actually used. They then combined these two measures into an F1-measure to give an overall grade to a system.

The lack of testing done on dynamic auto-completers helped me realize the need for a system that does what my second validation method will do. The testing done on most static auto-completers inspired the first validation method of random testing on existing code. The metrics they used are all similar and will help me define my metrics in a way that works best for this project. Lastly, although a lot of improvements were type-system specific, some could be used as ideas for completion approximation methods in my thesis.

\chapter{Implementation}

There are two major metrics to be considered when testing code completion. The first is how well a completer can get expected results near the top of the list. The second is how well a completer can keep failing results out of the list. I wrote a separate testing framework for each of these metrics, both of which use common base functionality.

\section{Foundation}

There are two main ways to tell whether code completion meets users' needs. One is to do user studies or get feedback from users, while another is to measure completion results against existing code. This testing framework takes the second approach.

In order to test against existing source, the source file must be tokenized. One way to do this is just to delimit on whitespace. However, since it is uncommon for data such as numbers and strings to use code completion, we decided to use the language grammar to only test valid identifiers.

I used Racket's built-in reader to tokenize existing Racket source. I take the results of the reader and store each identifier along with source location in a word structure. This identifier+location structure is what is used in all of the testing.

Since testing every token of every source file can take a very long time when the amount of source code is large, we used randomized testing to save time while still collecting representative data. To do this, I collect all of the identifiers from a file as described above and then randomly choose a percentage of those tokens to test against.

When an identifier from the original source is tested, the source file can be modified in several ways. We chose two simple ways to modify the source. The first way is to simply remove the token from the source file, which represents modifying or maintaining existing source code. The second way is to truncate the source file from the chosen identifier, which represents writing new source code.

\section{Ranker}

For a given token in a source file, the ranker will modify the file at that position, either by removing or truncating as described above, and then invoke an auto-completer to get a list of completion results. It then compares the list of results to the original token, which represents the programmer's intent. If the original token is found in the first five results, its rank is recorded. It is also noted whether the original token is found later in the list or not at all.

This method of testing completion results allows changes affecting the number and location of results to be checked quantifiably and consistently.

\section{Checker}

Similar to the ranker, the checker asks an auto-completer for a list of completions from a modified source file. However, instead of checking against the original token, the original token is replaced by one of the completion results, and the resulting program is compiled and run. If the program runs to completion, the completion results is recorded as a success. If the program fails to compile or terminates for any reason, the type of error is recorded.

This method allows auto-completers to test how top completion results fail for different methods of completion. With this information, targeted changes can help reduce certain types of failures.

\chapter{Verification}

To verify the rigor and correctness of the testing framework, we ran it against several large real-world Racket packages. Frog is a static website generator, mainly used for generating and updating blogs. Marketplace is a network-aware programming language that handles many problems inherent with distributed programming. Pfds is an implementation of many purely functional data structures written in Typed-Racket.

\section{Method Descriptions}

\subsection{Textual Heuristics}

Textual heuristics uses source code tokenization to determine which completions to suggest. Suggestions are prioritized based on proximity.

The implementation is fairly straightforward. We just separate based on the language's defined delimiters by processing the whole text file with the following regular expression: \scheme{[^\s()[]",'`#|\;]+}

The resulting strings and positions are combined into the word struct. When the completer is invoked at a specific position, these results are prioritized based on the following sort order:

\scheme{s(w1, w2) = |(w1 position) - (completer position)| < |(w2 position) - (completer position)|}

\subsection{Structural Heuristics}

The structure of the language can give good hints about how to filter and prioritize completion suggestions. The two ways we did this for Racket are by nesting level and by keywords and position.

For nesting level, we took each word returned by the textual heuristics method and attached nesting level information to it. We did this by starting at the beginning of the file and keeping track of how many parentheses and brackets had passed. The nesting level was incremented for each opening parenthesis or bracket and decremented for each closing.

This nesting information was then used to prioritize results. Suggestions were ordered based on the difference in nesting level between the current position at completion invocation and each suggestion.

For language keywords, a regular expression was used to find all function names based on the \scheme{define} and \scheme{let} keywords. We then determined whether the position of the completer invocation was in function application position. In Racket this is a simple check: if the position is directly after an opening parenthesis, it is in function application position. If this was the case, all function names we found previously were prioritized ahead of all other tokens.

\subsection{Macro Heuristics}

As we've discussed, macros can do arbitrary things to the source code. However, many macros come as part of the language, so we can take advantage of the knowledge of what to expect.

Because auto-completers are focused on identifiers more than functionality, it makes sense to limit macro heuristics to macros that introduce new identifiers. In Racket, the most common of these are \scheme{#lang}, \scheme{require}, and \scheme{define-struct}.

The implementation makes use of Racket namespaces and dynamic evaluation. For both \scheme{#lang} and \scheme{require}, we find macros of this kind using a regular expression and then \scheme{dynamic-require} them into a new empty namespace. The resulting list of mapped symbols in that namespace is the list of identifiers introduced by those macros.

With structs, we again find uses by processing the file with a regular expression. However, because there is no pre-defined form for dynamically creating structs, it is necessary to \scheme{eval} the resulting forms in a new namespace. Again, the resulting identifiers are queried from the namespace and added to the previous list.

\subsection{Macro Analysis}

The only sure way to know what identifiers are available in the presence of the problems listed above is to fully expand and link the program. The downside to this is that the source has to be compilable for this to work. The upside is that the information gained is the most accurate.

Because getting any results requires the file to compile without errors, we did not use the truncate source alteration method for any of these tests.

Racket source files can be expanded and compiled to byte code with \scheme{zo-parse}. The result of this process is .zo file that contains all the information needed to run the program.

The two pieces of information we used for the completer were required identifiers and top level (or global) identifiers. These are tokens that could most likely be used at any point in the program.

\subsection{Combined}

We hypothesized that a select combination of all the above methods would give the best results. Because all of the methods could just be called one after the other, the real problem was deciding how to order them.

We found the best results by putting nesting structural heuristics results first, then just appending the results of macro analysis in alphabetical order.

\section{Ranker Results}

Looking at the methods individually, the textual methods did a much better job than the macro methods at not only including expected tokens in the list of results, but also prioritizing them well. Even though macro heuristics and analysis suggested many of the correct tokens, almost none of the results were in the top five positions. This is mainly because textural heuristics are done on the source code, making different ordering algorithms easy to experiment with and use. On the other hand, macro heuristics and analysis are more meta by nature, not lending themselves to obvious sorting algorithms, leaving alphabetical the main alternative.

Among the textual methods, sorting by nesting level was by far the most effective. Of the tokens that correctly made it into the list of results, nearly 45% of them were ranked in the top five, as opposed to less than 25% for the other two textual methods.

Of the macro methods, macro analysis did roughly 25% better than macro heuristics. This makes sense as macro analysis is actually compiling the code to get fully expanded macros as well as all external dependencies. However, the fact that macro heuristics did so well in comparison makes it a good fallback for when the code is in an uncompilable state.

It is also interesting to note that truncating the text (as opposed to simply removing the token) adversely affected the textual methods as expected, but the macro methods were barely affected at all.

Since macro analysis did better in the macro camp and nesting level did better in the textual camp, we decided to combine the two. Nesting level did much better at ordering, so those results were ordered first, with macro analysis results appended. The results were that the results showing up in the top five spots stayed about the same, whereas the results that didn't show up at all were decreased by about 75%. This shows that the results provided by the two methods were significantly non-overlapping.

\section{Checker Results}

Because the checker checks the first five results for success or failure, it is expected that the success rate of the checker would to a certain degree follow the success rate of the ranker. Therefore, it is not surprising that the textual methods had a higher success rate than the macro methods.

It is interesting to note that, among the textual methods, the function keywords approach led to the least failures. This makes sense because it is not unlikely to have multiple functions that take the same number and type of arguments, thus allowing the program to run without crashing (although likely with incorrect end results).

The type of the top occurring error for each method is also instructive. For the textual methods, the top occurring error by far is using unbound identifiers. This makes sense because the completer is using every token in the file to present completion results, and many of them will not be bound at all or will be bound in a local context. The function keywords approach had the least percentage of unbound identifier failures (28% as opposed to 32% and 34%), which makes sense because functions are often defined at the top level, making them available anywhere in the module.

For the macro methods, the top occurring errors were syntax errors, which happened about twice as much as unbound identifier errors. This is instructive and expected because the point of the macro methods is to bring in globally available identifiers from external sources such as ones provided by the language itself or by imported modules, so incorrect use of them is most likely to be syntax related.

The results didn't show much difference between removing tokens and truncating the file as far as failures go. The one interesting thing to note is that the success percentage actually went up by truncating text for the function keywords method. This could mean that because the truncated file had fewer functions to choose from, the likelihood that one of them would match the calling function signature would be higher. However, it also suggests that functions are generally used after the definition, so truncating later functions tends to prune the number of incorrect functions.

\chapter{Conclusions}

Stuff

\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}

% vim: lbr
